{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>This is documentation for code written as part of the manuscript \"Data-driven fine-grained region discovery in the mouse brain with transformers\". </p>"},{"location":"#installation","title":"Installation","text":"<ul> <li><code>pip install git+github.com:abbasilab/celltransformer.git</code> or clone and pip install; alternatively use the Dockerfile, which uses <code>uv</code> to reduce build time.</li> </ul>"},{"location":"#getting-started-with-training-on-different-datasets","title":"Getting started with training on different datasets","text":"<p>Requirements are a CSV file with cell types and cell IDs corresponding to an <code>anndata</code> object with probe counts. To set these up for use with the code in this repo:</p> <ol> <li>make sure you provide the right cardinality (number of cell types) to the model using the <code>model</code> config. You can also do this using the <code>hydra</code> CLI by just passing (say you want to change the parameter <code>cell_cardinality</code> in the your <code>model.yaml</code> file): <code>python [SCRIPT.PY] +model.cell_cardinality=9000</code> (or whatever the value is)</li> <li>the code right now unfortunately assumes that the values in your anndata.X are <code>log1p</code> transformed and then uses <code>.exp()</code> in the implementation (<code>training/lightning_model.py</code>) to produce counts again for <code>scvi.NegativeBinomial</code> -- make sure you follow this convention or edit the training file for a different convention.</li> </ol> <p>To see a minimal example of this please see <code>notebooks/demo_celltransformer_onesection.ipynb</code>.</p>"},{"location":"#i-want-to-edit-the-anndata-merfish-probe-counts-and-csv-cell-metadata-to-work-with-this-codebase","title":"I want to edit the anndata (MERFISH probe counts) and CSV (cell metadata) to work with this codebase","text":"<ol> <li> <p>Provide in your CSV (case sensitive):</p> column name description <code>cell_type</code> integer encoded class label for the cell type of a given cell <code>cell_label</code> value that will be used to index the <code>anndata</code> object. Make sure it is of appropriate datatype because we do not perform any transformation on it (such as conversion to str or int) prior to indexing the <code>anndata</code> object. <code>brain_section_label</code> value that we will <code>.groupby()</code> on to select individual tissue sections to get the cells <code>x</code> spatial coordinate that will be used to identify neighbors. Must be in same units as <code>patch_size</code> argument (default in <code>hydra</code> configs is micron). <code>y</code> similar as <code>x</code> </li> <li> <p>Change paths in the <code>hydra</code> config file template (<code>scripts/config/data/mouse1.yaml</code>). If you do not do this, the script will almost certainly not work.</p> </li> <li>Make sure value of <code>patch_size</code> fits your <code>x</code> and <code>y</code> created in step (1) in the same YAML file.</li> <li>Adjust model parameters in <code>scripts/config/model/</code> for desired model, or adjust at runtime using <code>hydra</code> composition (see hydra docs). </li> <li>Set <code>config_path</code> in <code>hydra.main</code> decorator to the path and config file of interest (see template file <code>scripts/training/train_base.py</code>).</li> <li>Set up <code>wandb</code> parameters in <code>scripts/training/train_base.py</code>.</li> <li>Run with <code>python train_base.py</code>.</li> </ol>"},{"location":"#i-want-to-edit-this-at-the-hydra-config-level-or-in-the-starter-script-or-just-want-to-better-understand-the-above","title":"I want to edit this at the hydra config level or in the starter script (or just want to better understand the above)","text":"<ol> <li>Hydra config file setup: change data paths in <code>config/data</code> or create new data yaml file in that folder that has the same fields as the examples in that directory. Make sure to specify:<ul> <li><code>celltype_colname</code>: the column that gives the cell type of the cells in the dataset (if you are templating from the <code>train_aibs_mouse.py</code> file, which we recommend, we will use <code>sklearn.preprocessing.LabelEncoder</code> on this column in the base <code>train_aibs_mouse.py</code> file, so it doesn't matter if it's integer or string encoded. If you are not using that function, make sure the dataframe you pass to <code>CenterMaskSampler</code> has column <code>cell_type</code> (case sensitive default argument) which must integer encoded.)<ul> <li>NOTE: we are basically assuming you want to train on one mouse, so if there are multiple and there is a chance that one mouse out of multiple has some cell types that are not shared, you need to separately fit the LabelEncoder and then provide integer-encoded class labels (see <code>train_zhuang.py</code> for an example.)</li> </ul> </li> <li><code>cell_id_colname</code>: the column that gives an ID that we can use to lookup into the h5ad file for single cells' probe count profiles. Make sure this is of the same datatype as the row ID's in the <code>anndata</code> object (i.e. make sure that the ID isn't 12345: uint | int instead of 12345: str). </li> </ul> </li> <li>specify model architecture (depth, width etc.) configs in <code>config/model</code> yaml file</li> <li>implement the <code>load_data</code> method for <code>BaseTrainer</code> in <code>scripts/training/lightning_model.py</code> (see <code>train_zhuang.py</code> and <code>train_aibs_mouse.py</code> for reference)<ul> <li>in essence this is normalization: mapping spatial x and y coordinates in your data to \"x\" and \"y\" and scaling them, also normalizing cell type column names as described in (1), optionally. One example might be to filter control probes. <ul> <li>specifically you can look at <code>load_data</code> in <code>train_aibs_mouse</code> to see an example, but the dataloader code assumes that the spatial columns are <code>x</code> and <code>y</code>. </li> <li>we also don't automatically rescale the units of the <code>x</code> and <code>y</code> columns relative to the <code>patch_size</code> arguments. The idea is for the user to correctly scaled versions and to use code in <code>scripts/training/lightning_model.py:BaseTrainer.load_data</code> to set up the data loader with the logic you need for your data, and then pass a version of that to <code>celltransformer.data.CenterMaskSampler</code></li> </ul> </li> <li>for more information on the data and dataloader, see the data + dataloader page</li> </ul> </li> <li>alternatively just change the <code>data.patch_size</code> config value in <code>hydra</code> (see <code>scripts/config/data/</code>); as long as the desired patch size and spatial units in the dataframe are correctly scaled, then it will work</li> <li>add <code>wandb</code> project if desired to top level config in <code>config</code></li> <li>copy boilerplate for initiating training from <code>train_aibs_mouse.py</code> or <code>train_zhuang.py</code> (ie code in <code>main</code> that); make sure to specify correct config file in the <code>@hydra.main</code> decorator<ul> <li>for more information on this see the <code>hydra</code> docs.</li> </ul> </li> </ol> <p>The design of the dataloader object is:</p> <ol> <li>Store the dataframe (with cell metadata including x/y coordinates, cell types (integer encoded), and the unique identifier (referred to throughout the codebase as cell label) for each cell that we will to index into the anndata, as well as section IDs/groupings) along with the anndata <ul> <li>note there are some rudimentary checks to see if you have provided column names not found in the metadata dataframe. You do not need to have the metadata columns in the <code>anndata</code></li> </ul> </li> <li>Receive the string valued column names of the same (x/y coordinates, cell type column, section ID/groupings) and use them to index into the dataframe</li> <li>Store the cells corresponding to each column in a dictionary for later use (ie section_1: some_dataframe)</li> <li>The high level of the getitem is then to index the cell of interest (cell_i), lookup the neighbors based on the user-provided (see init for the CellTransformer model object) spatial threshold parameter, and then produce a <code>namedtuple</code> (<code>NeighborMetadata</code>) that has as attributes:<ul> <li><code>observed_expression</code>: a <code>numpy</code> array with dimensions (<code>n_cells</code> by <code>n_genes</code>)</li> <li><code>masked_expression</code>: a 1 by <code>n_genes</code> matrix</li> <li><code>masked_cell_type</code>: integer encoding for the cell type of the masked cell (cell_i)</li> <li><code>masked_expression</code>: a <code>n_cells</code> by <code>n_genes</code> matrix</li> <li><code>num_cells_obs</code>: number of masked cells</li> </ul> </li> <li>By default we will use the <code>celltransformer/data/loader_pandas.py:collate</code> function. It will loop over the list of <code>NeighborMetadata</code> and output a dictionary containing various concatenated data including the attention masks (keys <code>encoder_mask</code> and <code>pooling_mask</code>) to allow cells within neighborhoods to attend to each other across the batch and mask for attention pooling, respectively. See <code>forward</code> function of the CellTransformer model code to understand further operations.</li> </ol>"},{"location":"#core-code-components-and-usage","title":"Core code components and usage","text":""},{"location":"#config-management-with-hydra","title":"Config management with hydra","text":"<p>The main interface to the training code we wrote is through <code>hydra</code> (https://hydra.cc/), which is a configuration framework that uses yaml files to orchestrate and organize complex workflows. Please see the <code>hydra</code> documentation for more information.</p> <p>The pipeline controls the basic training operations through these yaml files and Pytorch Lightning.</p> <p>For example, <code>scripts/config/model/base.yaml</code> controls the parameters of the transformer itself, for example:</p> <pre><code>_target_: celltransformer.model.CellTransformer\nencoder_embedding_dim: 384\ndecoder_embedding_dim: 384\n\nencoder_num_heads: 8\ndecoder_num_heads: 8\nattn_pool_heads: 8\n\nencoder_depth: 4\ndecoder_depth: 4\n\ncell_cardinality: 384\neps: 1e-9\n\nn_genes: 500\nxformer_dropout: 0.0\nbias: True\nzero_attn: True\n</code></pre> <p>We can use hydra to directly instantiate this model (which we specify using the <code>_target_</code> attribute) by specifiying the object class, here <code>celltransformer.model.CellTransformer</code>. What this looks like in context is in following snippet:</p> <pre><code>cfg_path = 'config.yaml'\ncfg = OmegaConf.load(cfg_path) # same as above snippet\nmodel = hydra.utils.instantiate(cfg.model)\n\n# model will have 500 gene output decoder depth of 4, etc. and will be an instance of class `celltransformer.model.CellTransformer`\n</code></pre>"},{"location":"#composition-of-config-files-is-controlled-at-top-level-using-another-config","title":"Composition of config files is controlled at top-level using another config","text":"<p>An example of this composition at high level is the <code>scripts/config/example.yaml</code> file, which contains the settings used to train on the Allen Institute for Brain Science MERFISH data in the Allen Brain Cell Atlas. Note that \"mouse1.yaml\" refers to a file inside the <code>config/data</code> directory. Correspondingly there is a <code>config/model/base.yaml</code> file that is specified by the below config, which is found one-level-up (ie in the <code>config</code> directory). </p> <pre><code>defaults:\n  - _self_\n  - data: mouse1.yaml\n  - model: base.yaml\n  - optimization: base.yaml\n\ncheckpoint_dir: \nwandb_project: \nmodel_checkpoint: \nwandb_code_dir: \n</code></pre> <p>Where you can see we can group and order config components and define several high level attributes such as the checkpoint directory. You may like, however, to change these. For example including <code>wandb_project</code> will assume you can <code>wandb.login()</code> (see the wandb website for information on wandb and how to get a free account) and set this as the project. </p> <p>The <code>wandb_code_dir</code> argument will be used later to log the specific code used. </p> <p>For some files, a field may read: <code>???</code> indicating that field must be filled or <code>hydra</code> will error. </p> <p>Overall, fields in config files are accessible as <code>.[attribute]</code> in the DictConfig (from Omegaconf) object for example <code>config.model.n_genes</code>.</p> <p>Keep in mind that for dataset paths, all of them ought to be hardcoded in. Therefore, for datapaths in <code>config/data</code> these paths should be considered placeholders for you to fill in. I left in paths to explicitly indicate filepaths to the Zhuang and AIBS MERFISH data hosted on https://allen-brain-cell-atlas.s3.us-west-2.amazonaws.com/index.html.</p>"},{"location":"#training-on-the-aibs-merfish-data","title":"Training on the AIBS MERFISH data","text":"<p>The entrypoint to the training used for the Allen Institute for Brain Science MERFISH dataset (mouse 6388550) is in <code>scripts/train_aibs_mouse.py</code>, which uses <code>scripts/config/aibs1.yaml</code>. To run the code (assuming the package has been installed):</p> <ol> <li>download the data (use <code>scripts/download_aibs.sh</code>)</li> <li>edit <code>config/data/mouse1.yaml</code>, specifically: <pre><code>adata_path: './abc_dataset/C57BL6J-6388550-log2.h5ad'\nmetadata_path: './abc_dataset/cell_metadata_with_cluster_annotation.csv'\n</code></pre></li> <li>change whatever combination of checkpoint and <code>wandb</code> settings in <code>scripts/config/aibs1.yaml</code></li> <li>run the trainer script (<code>scripts/training/train_aibs_mouse.py</code>)</li> </ol> <pre><code>chmod +x scripts/download_aibs.sh\n./scripts/download_aibs.sh\npython scripts/training/train_aibs_mouse.py\n</code></pre> <p>If this is useful to you, please consider citing our preprint:</p> <pre><code>@ARTICLE{Lee2024-bh,\n  title    = \"Data-driven fine-grained region discovery in the mouse brain with\n              transformers\",\n  author   = \"Lee, Alex J and Dubuc, Alma and Kunst, Michael and Yao, Shenqin\n              and Lusk, Nicholas and Ng, Lydia and Zeng, Hongkui and Tasic,\n              Bosiljka and Abbasi-Asl, Reza\",\n  journal  = \"bioRxivorg\",\n  pages    = \"2024.05.05.592608\",\n  abstract = \"Technologies such as spatial transcriptomics offer unique\n              opportunities to define the spatial organization of the mouse\n              brain. We developed an unsupervised training scheme and novel\n              transformer-based deep learning architecture to detect spatial\n              domains across the whole mouse brain using spatial transcriptomics\n              data. Our model learns local representations of molecular and\n              cellular statistical patterns which can be clustered to identify\n              spatial domains within the brain from coarse to fine-grained.\n              Discovered domains are spatially regular, even with several\n              hundreds of spatial clusters. They are also consistent with\n              existing anatomical ontologies such as the Allen Mouse Brain\n              Common Coordinate Framework version 3 (CCFv3) and can be visually\n              interpreted at the cell type or transcript level. We demonstrate\n              our method can be used to identify previously uncatalogued\n              subregions, such as in the midbrain, where we uncover gradients of\n              inhibitory neuron complexity and abundance. Notably, these\n              subregions cannot be discovered using other methods. We apply our\n              method to a separate multi-animal whole-brain spatial\n              transcriptomic dataset and show that our method can also robustly\n              integrate spatial domains across animals.\",\n  month    =  jun,\n  year     =  2024,\n  language = \"en\"\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This documentation structure was copied largely from Patrick Kidger's <code>jaxtyping</code> docs.</p>"},{"location":"analysis/","title":"post-hoc analysis","text":"<p>See scripts <code>scripts/cluster.py</code> and <code>scripts/embeds_from_files.py</code>.</p> <p>Once the model is trained, see <code>embeds_from_files.py</code> for a minimal example of extracting embeddings from the AIBS data. </p> <p>K-means clustering can be run with example code at <code>cluster.py</code>. </p>"},{"location":"data/","title":"data + dataloader","text":""},{"location":"data/#dataloader-information","title":"Dataloader information","text":"<p>We use a packed sequence format, so the returns from this function are not in general <code>(batch_size, length, features)</code> tensor but a <code>(length, features)</code> matrix. </p> <p>Here is the docstring for the loader:</p> <p>Sampler that returns the gene expression matrices and cell-type identity vectors  for two groups of cells: the observed cells and the masked/reference cells. The observed cells  are the cells in the neighborhood of the reference cell (set by <code>patch_size</code>).</p> <p>We assume the type will be found at <code>cell_type_colname</code> and that <code>cell_id</code> can be used  succesfully to index into the adata object.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.</p> required <code>adata</code> <code>AnnData</code> <p>Expression-containing (as .X) anndata. We assume input will be log scaled.</p> required <code>patch_size</code> <code>Union[List[int], Tuple[int]]</code> <p>Size in arbitrary units for the neighborhood calculation.</p> required <code>cell_id_colname</code> <code>Optional[str]</code> <p>The column to use to index into the anndata, by default \"cell_label\"</p> <code>'cell_label'</code> <code>cell_type_colname</code> <code>Optional[str]</code> <p>The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"</p> <code>'cell_type'</code> <code>tissue_section_colname</code> <code>Optional[str]</code> <p>To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"</p> <code>'brain_section_label'</code> <code>max_num_cells</code> <code>Optional[Union[int, None]]</code> <p>How many cells to threshold at for the neighborhood size, by default None</p> <code>None</code> <code>indices</code> <code>Optional[Union[List[int], None]]</code> <p>Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with  <code>.iloc</code>, so it may be advisable to reset the index of the dataframe. By default None</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If adata is not an <code>ad.annData</code> object</p> <code>ValueError</code> <p>If metadata does not contain the necessary columns</p> <code>TypeError</code> <p>If metadata is not a pandas DataFrame</p> <code>ValueError</code> <p>If patch_size is not a tuple of length 2</p> <code>ValueError</code> <p>If metadata and adata are not the same length</p> <code>ValueError</code> <p>If metadata does not contain columns \"x\" and \"y\"</p> <p>The required data components are an <code>anndata</code> object with keys corresponding to columns in a dataframe of metadata, <code>metadata</code>. </p> <p>The dataframe must have:</p> <ul> <li>x: spatial coordinates [default: x]</li> <li>y: spatial coordinates [default: y]</li> <li>one column (<code>cell_id_colname</code>) which refers to the keys, these will be used directly to subset into the <code>anndata</code> object [default: cell_label]</li> <li>one column (<code>cell_type_colname</code>) which is a class encoded integer corresponding to the single cell classes from the non-spatial scRNA-seq data clustering [default: cell_type]</li> </ul> <p>The units of <code>patch_size</code> are not scaled internally, so they must \"match\" the units of <code>x</code> and <code>y</code>. </p> <p>What if you don't have a dataframe with those columns (and therefore is incorrectly formatted for <code>celltransformer.data.CenterMaskSampler</code>)? Your options are:</p> <ol> <li>just create a new version of the csv/tsv/etc. with the correct column names and metadata </li> <li>use the <code>scripts/training/lightning_model.py:BaseTrainer.load_data</code> function to preprocess your data in the format that will satisfy the conditions (see <code>scripts/training/train_aibs_mouse.py:load_data</code>) as an example. The motivations for this are covered in the TLDR in the main page, but we will also discuss it here. </li> </ol>"},{"location":"data/#expectations-for-inputs-into-celltransformerdatacentermasksampler-using-scriptstrainingtrain_aibs_mousepyload_data-as-an-example","title":"Expectations for inputs into <code>celltransformer.data.CenterMaskSampler</code>, using <code>scripts/training/train_aibs_mouse.py:load_data</code> as an example","text":"<ol> <li>we need columns <code>x</code> and <code>y</code> which match the units of <code>patch_size</code><ul> <li>e.g. if your patch size desired size is 10um, and your spatial dimensions are provided in nm, you would want to rescale either the patch size or the spatial dimensions. I elected to primarily rescale the spatial dimensions (for my own sanity) all to um, and so use the <code>load_data</code> function do do so.</li> </ul> </li> <li>we also need maybe to remap <code>cell_type_colname</code> to give </li> </ol> <p>Therefore, let's annotate the test code from <code>train_aibs_mouse.py:load_data</code>:</p> <pre><code># make sure to encode as str because for whatever reason\n# in the AIBS data, the anndata row ID's are string instead of int\nmetadata[\"cell_label\"] = metadata[\"cell_label\"].astype(str) \nmetadata[\"x\"] = metadata[\"x_reconstructed\"] * 100 # initially x_reconstructed in wrong units\nmetadata[\"y\"] = metadata[\"y_reconstructed\"] * 100\n\nmetadata = metadata[ # throw away nonessential columns\n    [\n        config.data.celltype_colname,\n        \"cell_label\", \n        \"x\",\n        \"y\",\n        \"brain_section_label\",\n    ]\n]\n\n# label_to_cls is just a thin wrapper over \n# `sklearn.preprocessing.LabelEncoder`\nmetadata[\"cell_type\"] = self.label_to_cls(\n    metadata[config.data.celltype_colname]\n)\n# make sure to integer encoder the classes \nmetadata[\"cell_type\"] = metadata[\"cell_type\"].astype(int)\n\nmetadata = metadata.reset_index(drop=True)\n\n# in the AIBS dataset some cells for which gene \n# expression was measured do not have metadata associated\n# so let's throw those out\nadata = adata[metadata[\"cell_label\"]]\n</code></pre> <p></p>"},{"location":"data/#outputs-of-__getitem__-and-collate","title":"Outputs of <code>__getitem__</code> and <code>collate</code>","text":"<p>These are the steps we implement so far:</p> <ol> <li>extract spatial neighbors (within some radius) for the cell of interest</li> <li>extract expression (from the <code>anndata</code> object) and the class encoding (<code>cell_type</code>) for each cell from the metadata dataframe you should have passed to <code>CenterMaskSampler</code></li> <li>partition them into two sets, which are simply lists in a <code>namedtuple</code>, see <code>celltransformer.data.NeighborhoodMetadata</code></li> </ol> <p>Note that we then need to stack these together across batches and create attention matrix masks, which is done in <code>celltransformer.data.loader_pandas.collate</code>. We create three attention mask matrices (we use a binary adjacency matrix):</p> Syntax Description <code>encoder_mask</code> allow the neighborhoods to only attend to each other <code>pooling_mask</code> pool into a single query token <code>decoder_mask</code> allow query token and decoding cell tokens to attend to each other <p>See the function documentation and the next section model for more information. </p> <p>My first-shot implementation of this workflow (in <code>pandas</code>) was ~1.3X faster than my next attempt, in <code>polars</code>. I welcome any feedback on why my <code>polars</code> code may have been suboptimal!</p>"},{"location":"model/","title":"model attention matrix format","text":""},{"location":"model/#formatting-data-for-the-model","title":"Formatting data for the model:","text":"<p>The model expects a packed sequence format. Because of the various operations of the network, in practice we need three attention mask matrices.</p> <ol> <li> <p>one matrix to mask the encoder </p> <ul> <li>at this stage, the model sees the \"observed\" cells (not the reference cell) and its various <code>&lt;cls&gt;</code>-like tokens</li> <li>we store these in blocks such that the first block of tokens (<code>n_toks</code> by <code>embed_dim</code>) are all the cells in the different neighborhoods, and the last <code>n</code> indices correspond to the -like tokens. Therefore the attention mask for this stage features blocks in the upper left and then spans in the bottom right that allow the <code>&lt;cls&gt;</code>-like tokens to attend to the cell tokens and vice versa. <p>Here is an example of this matrix:</p> <p></p> <p>In this batch of data we can see that the first 15 or so tokens correspond to the first neighborhood. The next 15-20 are in another neighborhood. Then we have a larger neighborhood which has almost 35-40 cells. </p> <p>The smaller lines allow the neighborhood tokens and the <code>&lt;cls&gt;</code>-like tokens to attend to each other. </p> <li> <p>When we pool the tokens together, we use a different mask:</p> <p></p> <ul> <li>since we have three neighborhoods, we have three pooling tokens that are True/1 (or False/0, depending on the coding of the attention matrix masking) in spans corresponding to the neighborhood identity.</li> </ul> </li> <li> <p>Finally, we then concatenate these to the decoding tokens corresponding to the reference cells; for this we need a third matrix:</p> <p></p> <p>Where you can see the first token (the first pooled neighborhood representation) and the fourth (the decoding token for the first neighborhood) can attend to each other, and vice-versa.</p> </li> <p>Otherwise the components are all standard <code>PyTorch</code>. This is likely not computationally optimal. If possible future iterations of this codebase ought to include more performant versions of the standard transformer operations.</p>"},{"location":"usage/","title":"Example training on Allen Institute for Brain Science MERFISH dataset","text":""},{"location":"usage/#dataset-details-and-dataloader-setup","title":"Dataset details and dataloader setup","text":""},{"location":"usage/#model-setup","title":"Model setup","text":""},{"location":"usage/#_1","title":"Example training on Allen Institute for Brain Science MERFISH dataset","text":""},{"location":"usage/#celltransformer.model.base.CellTransformer","title":"<code>CellTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>celltransformer/model/base.py</code> <pre><code>class CellTransformer(nn.Module):\n    def __init__(\n        self,\n        encoder_embedding_dim: int,\n        encoder_num_heads: int,\n        encoder_depth: int,\n        decoder_embedding_dim: int,\n        decoder_num_heads: int,\n        decoder_depth: int,\n        attn_pool_heads: Optional[int] = 8,\n        cell_cardinality: Optional[int] = 1024,\n        put_device: Optional[str] = \"cuda\",\n        eps: float = 1e-15,\n        n_genes: Optional[int] = 500,\n        xformer_dropout: Optional[float] = 0.0,\n        bias: Optional[bool] = False,\n        zero_attn: Optional[bool] = True,\n    ):\n        \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n        Parameters\n        ----------\n        encoder_embedding_dim : int\n        encoder_num_heads : int\n        encoder_depth : int\n        decoder_embedding_dim : int\n        decoder_num_heads : int\n        decoder_depth : int\n        attn_pool_heads : Optional[int], optional\n            Number attention pool heads, by default 8\n        cell_cardinality : Optional[int], optional\n            Cardinality/number of different cell types for embedding, by default 1024\n        put_device : Optional[str], optional\n            For later referencing where to put input tensors, by default 'cuda'\n        eps : float, optional\n            Stability additional constant for NB params, by default 1e-15\n        n_genes : Optional[int], optional\n            Number genes, by default 500\n        xformer_dropout : Optional[float], optional\n            Dropout %, by default 0.0\n        bias : Optional[bool], optional\n            Use bias or not, by default True\n        zero_attn : Optional[bool], optional\n            Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n        \"\"\"\n        super().__init__()\n\n        self.eps = eps\n\n        _feature_dim = encoder_embedding_dim // 2\n\n        self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n        self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n        self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n        self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n        self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n        self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n        self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                                  bias=False, zero_attn=True, bias_kv=True)\n\n        self.encoder = set_up_transformer_layers(\n            encoder_embedding_dim,\n            encoder_num_heads,\n            encoder_depth,\n            xformer_dropout,\n            bias=bias,\n            bias_kv=False,\n            zero_attn=zero_attn,\n        )\n        #self.encoder = torch.compile(self.encoder)\n\n        self.decoder = set_up_transformer_layers(\n            decoder_embedding_dim,\n            decoder_num_heads,\n            decoder_depth,\n            xformer_dropout,\n            bias=bias,\n            bias_kv=False,\n            zero_attn=zero_attn,\n        )\n        #self.decoder = torch.compile(self.decoder)\n\n        self.zinb_proj = ZINBProj(\n            embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n        )\n\n        nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n        self.put_device = put_device\n\n    def forward(self, data_dict: dict):\n        bs = data_dict[\"bs\"]\n\n        cells = data_dict[\"observed_cell_type\"].to(self.put_device, non_blocking=False)\n        expression = data_dict[\"observed_expression\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        num_hidden = len(\n            data_dict[\"masked_cell_type\"]\n        )  # will in practice be == bs but in case later want to train on multiple cells\n        hidden_expression = data_dict[\"masked_expression\"].to(\n            self.put_device, non_blocking=False, dtype=torch.float32\n        )\n        hidden_cells = data_dict[\"masked_cell_type\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        pooling_mask = data_dict[\"pooling_mask\"].to(self.put_device, non_blocking=False)\n        decoder_mask = data_dict[\"decoder_mask\"].to(self.put_device, non_blocking=False)\n        encoder_mask = data_dict[\"encoder_mask\"].to(self.put_device, non_blocking=False)\n\n        cls_tokens = self.cls_token.repeat_interleave(bs, dim=0)\n\n        cells_embed = self.encoder_cell_embed(cells)\n\n        expression_embed = self.expression_projection(expression)\n        cells_embed = torch.cat((cells_embed, expression_embed), dim=1)\n        cells_embed = torch.cat((cells_embed, cls_tokens), dim=0)\n\n        cells_embed = self.proj_norm(cells_embed)\n\n        cells_embed = self.encoder(cells_embed, mask=encoder_mask)\n\n        attn_pool = self.attn_pool(cells_embed, pooling_mask, bs)\n\n        decoding_queries = self.decoder_cell_embed(hidden_cells)\n\n        cells_embed = torch.cat((attn_pool, decoding_queries), dim=0)\n\n        cells_embed = self.decoder(cells_embed, mask=decoder_mask)\n        ref_cell_embed = cells_embed[-bs:]\n\n        with torch.autocast(dtype=torch.float32, device_type=self.put_device):\n            # sometimes bfloat doesn't work here for lgamma backward (in zinb, otherwise will error)\n            zinb_params = self.zinb_proj(ref_cell_embed)\n\n        cls_toks = cells_embed[-(num_hidden + bs) : -num_hidden]\n\n        return dict(\n            zinb_params=zinb_params,\n            neighborhood_repr=cls_toks,\n            hidden_expression=hidden_expression,\n        )\n</code></pre>"},{"location":"usage/#celltransformer.model.base.CellTransformer.__init__","title":"<code>__init__(encoder_embedding_dim, encoder_num_heads, encoder_depth, decoder_embedding_dim, decoder_num_heads, decoder_depth, attn_pool_heads=8, cell_cardinality=1024, put_device='cuda', eps=1e-15, n_genes=500, xformer_dropout=0.0, bias=False, zero_attn=True)</code>","text":"<p>An encoder-decoder model with attention pooling prior to decoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_embedding_dim</code> <code>int</code> required <code>encoder_num_heads</code> <code>int</code> required <code>encoder_depth</code> <code>int</code> required <code>decoder_embedding_dim</code> <code>int</code> required <code>decoder_num_heads</code> <code>int</code> required <code>decoder_depth</code> <code>int</code> required <code>attn_pool_heads</code> <code>Optional[int]</code> <p>Number attention pool heads, by default 8</p> <code>8</code> <code>cell_cardinality</code> <code>Optional[int]</code> <p>Cardinality/number of different cell types for embedding, by default 1024</p> <code>1024</code> <code>put_device</code> <code>Optional[str]</code> <p>For later referencing where to put input tensors, by default 'cuda'</p> <code>'cuda'</code> <code>eps</code> <code>float</code> <p>Stability additional constant for NB params, by default 1e-15</p> <code>1e-15</code> <code>n_genes</code> <code>Optional[int]</code> <p>Number genes, by default 500</p> <code>500</code> <code>xformer_dropout</code> <code>Optional[float]</code> <p>Dropout %, by default 0.0</p> <code>0.0</code> <code>bias</code> <code>Optional[bool]</code> <p>Use bias or not, by default True</p> <code>False</code> <code>zero_attn</code> <code>Optional[bool]</code> <p>Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True</p> <code>True</code> Source code in <code>celltransformer/model/base.py</code> <pre><code>def __init__(\n    self,\n    encoder_embedding_dim: int,\n    encoder_num_heads: int,\n    encoder_depth: int,\n    decoder_embedding_dim: int,\n    decoder_num_heads: int,\n    decoder_depth: int,\n    attn_pool_heads: Optional[int] = 8,\n    cell_cardinality: Optional[int] = 1024,\n    put_device: Optional[str] = \"cuda\",\n    eps: float = 1e-15,\n    n_genes: Optional[int] = 500,\n    xformer_dropout: Optional[float] = 0.0,\n    bias: Optional[bool] = False,\n    zero_attn: Optional[bool] = True,\n):\n    \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n    Parameters\n    ----------\n    encoder_embedding_dim : int\n    encoder_num_heads : int\n    encoder_depth : int\n    decoder_embedding_dim : int\n    decoder_num_heads : int\n    decoder_depth : int\n    attn_pool_heads : Optional[int], optional\n        Number attention pool heads, by default 8\n    cell_cardinality : Optional[int], optional\n        Cardinality/number of different cell types for embedding, by default 1024\n    put_device : Optional[str], optional\n        For later referencing where to put input tensors, by default 'cuda'\n    eps : float, optional\n        Stability additional constant for NB params, by default 1e-15\n    n_genes : Optional[int], optional\n        Number genes, by default 500\n    xformer_dropout : Optional[float], optional\n        Dropout %, by default 0.0\n    bias : Optional[bool], optional\n        Use bias or not, by default True\n    zero_attn : Optional[bool], optional\n        Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n    \"\"\"\n    super().__init__()\n\n    self.eps = eps\n\n    _feature_dim = encoder_embedding_dim // 2\n\n    self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n    self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n    self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n    self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n    self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n    self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n    self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                              bias=False, zero_attn=True, bias_kv=True)\n\n    self.encoder = set_up_transformer_layers(\n        encoder_embedding_dim,\n        encoder_num_heads,\n        encoder_depth,\n        xformer_dropout,\n        bias=bias,\n        bias_kv=False,\n        zero_attn=zero_attn,\n    )\n    #self.encoder = torch.compile(self.encoder)\n\n    self.decoder = set_up_transformer_layers(\n        decoder_embedding_dim,\n        decoder_num_heads,\n        decoder_depth,\n        xformer_dropout,\n        bias=bias,\n        bias_kv=False,\n        zero_attn=zero_attn,\n    )\n    #self.decoder = torch.compile(self.decoder)\n\n    self.zinb_proj = ZINBProj(\n        embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n    )\n\n    nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n    self.put_device = put_device\n</code></pre>"},{"location":"zhuang_abc/","title":"Training on Zhuang lab data","text":"<p>In order to train on the Zhuang lab data please see the config files labeled \"Zhuang\".</p> <p>The setup is more or less the same as for the AIBS case. One easy solution is to simply load each metadata / <code>anndata</code> object indepently as a <code>celltransformer.data.SimpleMaskSampler</code> object and concatenate them together post-hoc using <code>torch.utils.data.ConcatDataset</code>. This is what we do.</p> <p>For reference, here is a snippet of code from the <code>ZhuangTrainer.load_data</code> function (see <code>scripts/training/train_zhuang.py</code>) and some annotation by me:</p> <pre><code>def load_data(self, config: DictConfig):\n\n    all_dfs = []\n    all_cls = set()\n\n    for df_path in config.data.metadata_path: \n\n        # We loop over all the dataframes first, so we can generate a consistent list \n        of all the celltypes in the datasets.\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", pd.errors.DtypeWarning)\n            metadata = pd.read_csv(df_path)\n\n        metadata[\"x\"] = metadata[\"x\"] * 100\n        metadata[\"y\"] = metadata[\"y\"] * 100\n\n        metadata = metadata.reset_index(drop=True)\n\n        all_cls.update(metadata[config.data.celltype_colname].unique())\n        all_dfs.append(metadata)\n\n    le = LabelEncoder()\n    le.fit(sorted(all_cls))\n\n\n    # Now that we have this (`le` consistent encoder) we can use this as we loop again \n    over the metadata/anndata pairs, creating for each one a `CenterMaskSampler` pair \n    and appending to the `trn_samplers` and `valid_samplers` lists.\n\n    trn_samplers = []\n    valid_samplers = []\n\n    for df, anndata_path in zip(all_dfs, config.data.adata_path):\n        df[\"cell_type\"] = le.transform(df[config.data.celltype_colname])\n        df[\"cell_type\"] = df[\"cell_type\"].astype(int)\n\n        df['cell_label'] = df['cell_label'].astype(str)\n\n        df = df[['cell_type', 'cell_label', 'x', 'y', 'brain_section_label']]\n\n        adata = ad.read_h5ad(anndata_path)\n        adata = adata[df[\"cell_label\"]]\n\n        train_indices, valid_indices = train_test_split(\n            range(len(adata)), train_size=config.data.train_pct\n        )\n\n        train_sampler = CenterMaskSampler(\n            metadata=df,\n            adata=adata,\n            patch_size=config.data.patch_size,\n            cell_id_colname=config.data.cell_id_colname,\n            cell_type_colname=\"cell_type\",\n            tissue_section_colname=config.data.tissue_section_colname,\n            max_num_cells=config.data.neighborhood_max_num_cells,\n            indices=train_indices,\n        )\n\n        valid_sampler = CenterMaskSampler(\n            metadata=df,\n            adata=adata,\n            patch_size=config.data.patch_size,\n            cell_id_colname=config.data.cell_id_colname,\n            cell_type_colname=\"cell_type\",\n            tissue_section_colname=config.data.tissue_section_colname,\n            max_num_cells=config.data.neighborhood_max_num_cells,\n            indices=valid_indices,\n        )\n\n        trn_samplers.append(train_sampler)\n        valid_samplers.append(valid_sampler)\n\n    train_loader = torch.utils.data.DataLoader(\n        torch.utils.data.ConcatDataset(trn_samplers),\n        batch_size=config.data.batch_size,\n        num_workers=config.data.num_workers,\n        pin_memory=False,\n        shuffle=True, \n        collate_fn=collate,\n        prefetch_factor=4, # muddling with this a bit can improve performance, depends on your setup\n\n    )\n</code></pre>"}]}